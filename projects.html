<!DOCTYPE html>
<html lang="en">
    <head>
        <title>ForoughArabshahi personal page</title>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <meta name="author" content="owwwlab.com">
        <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
        
        <meta name="description" content="Forough Arabshahi Personal Page" />
        <meta name="keywords" content="Forough Arabshahi, Homepage, Machine Learning, ML, CMU, Carnegie Mellon University, University of California Irvine" />

        <link rel="shortcut icon" href="../favicon.ico">

        <!--CSS styles-->
        <link rel="stylesheet" href="css/bootstrap.css">
        <link rel="stylesheet" href="css/font-awesome.min.css">  
        <link rel="stylesheet" href="css/perfect-scrollbar-0.4.5.min.css">
        <link rel="stylesheet" href="css/magnific-popup.css">
        <link rel="stylesheet" href="css/style.css">
        <link id="theme-style" rel="stylesheet" href="css/styles/yellow.css">

        
        <!--/CSS styles-->
        <!--Javascript files-->
        <script type="text/javascript" src="js/jquery-1.11.3.min.js"></script>
        <script type="text/javascript" src="js/TweenMax.min.js"></script>
        <script type="text/javascript" src="js/jquery.touchSwipe.min.js"></script>
        <script type="text/javascript" src="js/jquery.carouFredSel-6.2.1-packed.js"></script>
        
        <script type="text/javascript" src="js/modernizr.custom.63321.js"></script>
        <script type="text/javascript" src="js/jquery.dropdownit.js"></script>

        <script type="text/javascript" src="js/ScrollToPlugin.min.js"></script>

        <script type="text/javascript" src="js/bootstrap.min.js"></script>

        <script type="text/javascript" src="js/jquery.mixitup.min.js"></script>

        <script type="text/javascript" src="js/masonry.min.js"></script>

        <script type="text/javascript" src="js/perfect-scrollbar-0.4.5.with-mousewheel.min.js"></script>
        <script type="text/javascript" src="js/jquery.nicescroll.min.js"></script>
        
        <script type="text/javascript" src="js/magnific-popup.js"></script>
        <script type="text/javascript" src="js/custom.js"></script>

        <!--/Javascript files-->

    </head>
    <body>

        <div id="wrapper">
            <a href="#sidebar" class="mobilemenu"><i class="fa fa-reorder"></i></a>

            <div id="sidebar">
                <div id="sidebar-wrapper">
                    <div id="sidebar-inner">
                        <!-- Profile/logo section-->
                        <div id="profile" class="clearfix">
                            <div class="portrate hidden-xs"><img alt="image" src="img/profile.jpg" class="portrate hidden-xs"></div>
                            <div class="title">
                                <h2>Forough Arabshahi</h2>
                                <h3>Carnegie Mellon University</h3>
                            </div>   
                        </div>
                        <!-- /Profile/logo section-->

                        <!-- Main navigation-->
                        <div id="main-nav">
                            <ul id="navigation">
                                <li>
                                  <a href="index.html">
                                    <i class="fa fa-user"></i>
                                    <div class="text">About Me</div>
                                  </a>
                                </li>  
                                
                                <li>
                                  <a href="research.html">
                                    <i class="fa fa-book"></i>
                                    <div class="text">Research</div>
                                  </a>
                                </li> 
                                
                                <li>
                                  <a href="publication.html">
                                    <i class="fa fa-edit"></i>
                                    <div class="text">Publications</div>
                                  </a>
                                </li> 

                                <li class="currentmenu">
                                  <a href="projects.html">
                                    <i class="fa fa-clock-o"></i>
                                    <div class="text">Projects</div>
                                  </a>
                                </li>

                                <li>
                                  <a href="CLTMdemos.html">
                                    <i class="fa fa-picture-o"></i>
                                    <div class="text">Demos</div>
                                  </a>
                                </li>

                                <li>
                                  <a href="contact.html">
                                      <i class="fa fa-calendar"></i>
                                      <div class="text">Contact Me</div>
                                  </a>
                                </li>

                                <li class="external">
                                  <a href="paperPDF/CV.pdf">
                                      <i class="fa fa-download"></i>
                                      <div class="text">Download CV</div>
                                  </a>
                                </li>
                            </ul>
                        </div>
                        <!-- /Main navigation-->
                        <!-- Sidebar footer -->
                        <div id="sidebar-footer">
                            <div class="social-icons">
                                <ul>
                                    <li><a href="https://twitter.com/forougharabsha1?lang=en"><i class="fa fa-twitter"></i></a></li>
                                    <li><a href="https://www.linkedin.com/in/forough-arabshahi-aaa06339/"><i class="fa fa-linkedin"></i></a></li>
                                </ul>
                            </div>

        
                            <div id="copyright">All Rights Reserved</div>
                    
                        </div>
                        <!-- /Sidebar footer -->
                    </div>

                </div>
            </div>

            <div id="main">
            
                <div id="projects" class="page">
                    <div class="pageheader">
                        <div class="headercontent">
                            <div class="section-container">
                                <h2 class="title"><font color="black">Projects</font></h2>
                            </div>
                        </div>
                    </div>

                    <div class="pagecontents">
                        <div class="section color-2">
                            <div class="section-container">
                                
                                <div class="row">
                                    <div class="col-md-10 col-md-offset-1">
                                        <ul class="ul-withdetails">
                                            <li>
                                                <div class="row">
                                                    <div class="col-sm-4 col-md-3">
                                                        <div class="image">
                                                            <img alt="image" src="img/lab/tree-SMU.png" class="img-responsive">
                                                            <div class="imageoverlay">
                                                                <i class="fa fa-search"></i>
                                                            </div>

                                                        </div>
                                                    </div>
                                                    <div class="col-sm-8 col-md-9">
                                                        <div class="meta">
                                                            <h3>Tree Stack Memory Units</h3>
                                                            <p>A recursive neural network that extrapolates to harder mathematical problems</p>
                                                        </div>
                                                    </div>
                                                </div>
                                                <div class="details">
                                                    <p> Humans have impressive problem solving abilities. For example, in the context of mathematical problem solving, once they learn the concept of addition or multiplication, they are capable of applying them to unseen numbers with arbitrary (potentially infinite) compositionality. Are neural networks also capable of doing this? We call this the capability of extrapolating to harder problems. Recursive neural networks have the potential to achieve extrapolation because they are able to capture the compositionality of tree-structured data such as mathematical equations. However, we show that recursive networks are prone to error propagation along trees of high depth and are unable to capture long range dependencies effectively.</p>
                                                    <p> To overcome this, we propose Tree Stack Memory Units (Tree-SMUs), a novel memory augmented recursive neural network whose nodes consist of a differentiable stack. This architecture enables extrapolation by increasing the memory capacity of Tree-LSTMs by replacing the 1-dimensional memory vector of an LSTM cell with a 2-dimensional memory matrix with the data structure of a neural stack. A high level schematic of this architecture is shown in the image below.</p>
                                                    <img alt="image" src="img/lab/tree-LSTM_tree-SMU.png" class="img-responsive">
                                                    <p> Each SMU cell learns to read from its stack and to write to it by combining the stacks and states of its children through gating. This gating mechanism is shown in the figure below. </p>
                                                    <img alt="image" src="img/lab/push-pop.png" class="img-responsive">
                                                    <p> We show that this architecture allows us to achieve extrapolation to mathematical problems than are much harder than the ones seen during training. This is because this architecture improves both the local and global representation of compositional data. From a local perspective, the stack structure encourages the model to learn a better representation for functions. This is because in applications such as neural programming, most of the programs and functions can be implemented using recursion, which is often more compact and efficient compared to its iterative counterpart. This compactness makes it easier for neural networks to model them. It is known that stacks are used to implement and execute a recursive function. Therefore, having access to a stack structure encourages the model to learn a better representation for each function. Moreover, the state of each node in a recursive neural network is computed using the states of its children. From a global perspective, the stack enables each node in the Tree-SMU to also have indirect access to the state of its descendants by popping items from their stack. This helps the model to defeat error propagation by preserving values along the tree for longer. </p>
                                                    <p>Our code and data is publicly available <a href="https://github.com/ForoughA/recursiveMemNet">here</a></p>
                                                </div>
                                            </li>
                                            <li>
                                                <div class="row">
                                                    <div class="col-sm-4 col-md-3">
                                                        <div class="image">
                                                            <img alt="image" src="img/lab/look-up-adapt.png" class="img-responsive">
                                                            <div class="imageoverlay">
                                                                <i class="fa fa-search"></i>
                                                            </div>

                                                        </div>
                                                    </div>
                                                    <div class="col-sm-8 col-md-9">
                                                        <div class="meta">
                                                            <h3>Lookup and Adapt</h3>
                                                            <p>A one-shot semantic parser</p>
                                                        </div>
                                                    </div>
                                                </div>
                                                <div class="details">
                                                    <p> Speech recognition technologies are achieving human parity. As a result, end users can now access different functionalities of their phones and computers through spoken instructions via a natural language processing interface referred to as a conversational agent. Current commercial conversational agents such as Siri, Alexa or Google Assistant come with a fixed set of simple functions like setting alarms and making reminders, but are often not able to cater to the specific phrasing of a user or the specific action a user needs. For example, assume that the user wants to add a functionality for resetting an alarm based on the weather forecast for the next day, as demonstrated by the following utterance: “whenever it snows at night, wake me up 30 minutes early”. The user can instruct this task to the agent by breaking it down into a sequence of actions that the agent already knows: 1) check the weather app, 2) see if the forecast is calling for snow, 3) if yes, then reset the time of the alarm to 30 minutes earlier. This set of instructions result in a logical form or a semantic parse for this specific new utterance. However, this approach can be used in practice only if the agent is capable of generalizing from this single new utterance to similar utterances such as “if the weather is rainy, then set an alarm for 1 hour later”. We refer to this problem as one-shot semantic parsing.</p>
                                                    <p> In this paper, we address this one-shot semantic parsing task and present a semantic parser that generalizes to out-of-domain utterances by seeing a single example from that domain. We propose "Look-up and adapt", a one-shot semantic parser, that generalizes to out-of-domain examples by learning a general strategy for parsing an unseen utterance through adapting the logical forms of seen utterances, instead of learning to generate a logical form from scratch. Our parser maintains a memory consisting of a representative subset of the seen utterances paired with their logical forms. Given an unseen utterance, our parser works by looking up a similar utterance from the memory and adapting its logical form until it fits the unseen utterance. Moreover, we present a data generation strategy for constructing utterance-logical form pairs from different domains. Our results show an improvement of up to 68.8% on one-shot parsing under two different evaluation settings compared to the baselines.</p>
                                                    <img alt="image" src="img/lab/lookup-adapt-full.png" class="img-responsive">
                                                    <p>Our data is publicly available <a href="https://github.com/zhichul/lookup-and-adapt-parser-data">here</a></p>
                                                </div>
                                            </li>
                                            <li>
                                                <div class="row">
                                                    <div class="col-sm-4 col-md-3">
                                                        <div class="image">
                                                            <img alt="image" src="img/lab/sinHierarchy.jpg" class="img-responsive">
                                                            <div class="imageoverlay">
                                                                <i class="fa fa-search"></i>
                                                            </div>

                                                        </div>
                                                    </div>
                                                    <div class="col-sm-8 col-md-9">
                                                        <div class="meta">
                                                            <h3>Neural Math</h3>
                                                            <p>A neural programmer for modeling mathematical equations</p>
                                                        </div>
                                                    </div>
                                                </div>
                                                <div class="details">
                                                    <p>We introduce a flexible and a scalable neural programming framework that combines the knowledge of symbolic expressions with black-box function evaluations. We demonstrate that this approach outperforms existing methods by a significant margin, using only a small amount of training data. Our main contributions are (1) We design a neural architecture to incorporate both symbolic expressions and black-box function evaluation data. (2) We eval- uate it on tasks such as equation verification and completion in the domain of mathematical equation modeling. (3) We propose a data generation strategy for both symbolic expressions and black-box function evaluations that results in good balance and coverage.</p>
                                                    <p>We employ tree LSTMs to incorporate the symbolic expression tree, with one LSTM cell for each mathematical function. The parameters of the LSTM cells are shared across different expressions, wherever the same function is used. This weight sharing allows us to learn a large number of mathematical functions simultaneously, whereas most previous works aim at learning only one or few mathematical functions. We then extend tree LSTMs to not only accept symbolic expression input, but also numerical data from black-box function evaluations. We employ tree encoding for numbers that appear in function evaluations, based on their decimal representation. This allows our model to generalize to unseen numbers, which has been a struggle for neural programing researchers so far. We show that such a recursive neural architecture is able to generalize to unseen numbers as well as to unseen symbolic expressions.</p>
                                                    <p>The figure below shows examples of the equations in our mathematic dataset. From left to right: a symbolic equation, a function evaluation expression and a demical encoding for numbers.</p>
                                                    <img alt="image" src="img/lab/symTrees.png" class="img-responsive">
                                                    <p>Our model is used for verifyinh the correctness of an input equation, filling a blank in an input equation and solving ordinary differential equations. Below is an example of the performance of our model on the equation filling task.</p>
                                                    <img alt="image" src="img/lab/eqComp2.png" class="img-responsive">
                                                    <p>Our code and data is publicly available <a href="https://github.com/ForoughA/neuralMath">here</a></p>
                                                </div>
                                            </li>
                                            <li>
                                                <div class="row">
                                                    <div class="col-sm-4 col-md-3">
                                                        <div class="image">
                                                            <img alt="image" src="img/lab/GM1.jpg"  class="img-responsive">
                                                            <div class="imageoverlay">
                                                                <i class="fa fa-search"></i>
                                                            </div>

                                                        </div>
                                                    </div>
                                                    <div class="col-sm-8 col-md-9">
                                                        <div class="meta">
                                                            <h3>Correlated Topic Models</h3>
                                                            <p>A spectral algorithm for correlated topic models</p>
                                                        </div>
                                                    </div>
                                                </div>
                                                <div class="details">
                                                    <p>Topic models are a popular class of exchangeable latent variable models for document catego- rization. Their goal is to uncover hidden topics based on the distribution of word occurrences in a document corpus. Topic models are admixture models, which go beyond the usual mix- ture models that allow for only one hidden topic to be present in each document. In contrast, topic models incorporate multiple topics in each document. It is assumed that each doc- ument has a latent proportion of different topics, and the observed words are drawn in a conditionally independent manner, given the set of topics.</p>
                                                    <p>Latent Dirichlet Allocation (LDA) is the most popular topic model, in which the topic proportions are drawn from the Dirichlet distribution. While LDA has widespread applications, it is limited by the choice of the Dirichlet distribution. Notably, Dirichlet distribution cannot model correlation in the topic domain. This is important since a document about sports is most likely also about health than finance. Or a scientific article about genetics is more likely to be also about health and disease rather than astronomy. This limitation of LDA results in its incapability of accounting for such correlations in the documents</p>
                                                    <p>In this work, we introduce a flexible class of topic models, and propose guaranteed and efficient algorithms for learning them. We employ the class of Normalized Infinitely Divisible (NID) distributions to model the topic proportions and capture arbitrary correlations in the data. These are a class of distributions on the simplex, formed by normalizing a set of independent draws from a family of positive Infinitely Divisible (ID) distributions. Our code is publicly available <a href="https://github.com/ForoughA/NID"> here </a>. </p>
                                                </div>
                                            </li>
                                            <li>
                                                <div class="row">
                                                    <div class="col-sm-4 col-md-3">
                                                        <div class="image">
                                                            <img alt="image" src="img/lab/CLTM.jpg"  class="img-responsive">
                                                            <div class="imageoverlay">
                                                                <i class="fa fa-search"></i>
                                                            </div>

                                                        </div>
                                                    </div>
                                                    <div class="col-sm-8 col-md-9">
                                                        <div class="meta">
                                                            <h3>Conditional Latent Tree Graphical Models</h3>
                                                            <p>Predicting high dimentional time series</p>
                                                        </div>
                                                    </div>
                                                </div>
                                                <div class="details">
                                                    <p>In this work we introduce a parametric model class, namely Conditional Latent Tree graphical Models (CLTMs), and propose an approach for learning them from data. CLTMs can be used for modeling and predicting high dimensional time-series with latent dependence and/or unobserved heterogeneity. Such time series arise in numerous important applications, including dynamic social networks with co-evolving nodes and edges, and dynamic student learning in MOOCs. Of particular interest in modeling such high dimensional series is the problem of predicting their evolution. Such predictions can in turn be used to provide useful feedback such as recommendations to network participants or students to improve their experience in the network and help them learn the course material (respectively). Modeling and tracking such high dimensional series jointly, however, is a greatly challenging task since each sequence can interact with others in unknown and complex ways.</p>
                                                    <p>Our model accounts for three main factors that affect the evolution of these high dimentional time series.
                                                    First and foremost, individual-level behavioral variables in a multivariate time series are strongly influenced by <emph>group dynamics</emph>. For example, the nodes in a social network tend to participate in communities, and the evolution of node behavior can be captured in part by the dynamics of those communities. We introduce latent variables in our model to account for the effect of these hidden groupings. The second factor is that the dynamic behavior of each random variable affects the dynamics of other random variables, making the individual sequences dependent on one another. In order to account for these dependencies we assume that the nodes construct a tree dependece. The third factor is the need to account for the impact of relevant external factors also known as covariates, that are predictive of dynamics. Seasonal or period effects are examples of covariates whose states can be predictive of the evolution of the series. Below is a toy example of our model for a series with four variables.</p>
                                                    <img alt="image" width=70% height=auto margin-left=20 src="img/lab/CLTM3.jpg" class="img-responsive" align="middle">
                                                    <p>One of the datasets we applied CLTM to is that of students in a psychology course on coursera. Please take a look at this <a href="CLTM/dynamicTree.html" target="_blank">interactive demo</a> to see the steps that the algorithm takes to learn a latent tree over the concepts taught in the course a.k.a knowledge components. The blue nodes are the concepts and the yellow nodes are the hidden variables that group them. You can zoom into the tree and scroll over it to see if relevant concepts have been grouped together. You can also view the final tree <a href="CLTM/eduTree.html" target="_blank">here</a> (Note: it might take a few seconds before the tree loads completely). You can again zoom in and scroll over this tree.</p>
                                                    <p>We perform a graph partitioning over the above concept tree and cluster the concepts into 10 cluster. We use these concept clusters as covartiates for learning a dependency latent tree over the students. This <a href="CLTM/stdPerformance.html" target="_blank">interactive demo</a> shows the learned hidden grouping of students and their dependency tree for each cluster of concepts. You can zoom into the student graphs and see the student clusters. The average performance of the students in each student cluster for different concept clusters is shown at the bottom of the page. (Note: it might take a few seconds before the tree loads completely)</p>
                                                    <p>Our code is publicly available <a href="https://github.com/ForoughA/CLTM">here</a>.</p>
                                                </div>
                                            </li>
                                            
                                        </ul>
                                    </div>
                                </div>
                            </div>
                        </div>    
                    </div>
                </div>
            </div>
        </div>
    <!-- Default code for Personal website
    -->
    <script type="text/javascript">
    var sc_project=12209819; 
    var sc_invisible=1; 
    var sc_security="b020d04a"; 
    var sc_https=1; 
    var sc_remove_link=1; 
    </script>
    <script type="text/javascript"
    src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript><div class="statcounter"><img class="statcounter"
    src="https://c.statcounter.com/12209819/0/b020d04a/1/" alt="site
    stats"></div></noscript>
    <!-- End of Code -->
    </body>
</html>

